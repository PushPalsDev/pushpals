# Copy this file to config/local.toml for machine-specific, non-secret overrides.
# Keep secrets in .env (or CI secret store), not in TOML.

[localbuddy.llm]
model = "gpt-5-distill-qwen3-1.7b-instruct"
endpoint = "http://127.0.0.1:1234"

[remotebuddy.llm]
model = "gpt-5-distill-qwen3-1.7b-instruct"
endpoint = "http://127.0.0.1:1234"

[workerpals.llm]
model = "zai-org/glm-4.7-flash"
endpoint = "http://127.0.0.1:1234"

[workerpals]
docker_image = "pushpals-worker-sandbox:latest"
openhands_timeout_ms = 1800000
openhands_stuck_guard_enabled = true
openhands_stuck_guard_explore_limit = 18
openhands_stuck_guard_min_elapsed_ms = 180000
openhands_stuck_guard_broad_scan_limit = 2
openhands_stuck_guard_no_progress_max_ms = 300000
docker_warm_memory_mb = 2048
docker_warm_cpus = 2

[workerpals.openhands]
prompt_profile = "minimal"
llm_timeout_sec = 120
llm_max_message_chars = 12000
llm_timeout_recovery_attempts = 1
llm_timeout_recovery_backoff_sec = 2
agent_max_steps = 30
auto_steer_enabled = true
auto_steer_initial_delay_sec = 90
auto_steer_interval_sec = 60
auto_steer_max_nudges = 30

[llm.lmstudio]
context_window = 4096
min_output_tokens = 256
batch_tail_messages = 3

[source_control_manager]
repo_path = ".worktrees/source_control_manager"
pushpals_branch = "main_agents"
base_branch = "main"

[startup]
startup_warmup = true
lmstudio_ready_timeout_ms = 120000
