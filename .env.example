# ─── PushPals Environment ─────────────────────────────────────────────
# Copy to .env and fill in values.  Bun auto-loads .env at the repo root.
# ─────────────────────────────────────────────────────────────────────

# ─── Data ────────────────────────────────────────────────────────────
# Base directory for SQLite databases (default: <project_root>/outputs/data)
# The entire outputs/ dir is gitignored.
# PUSHPALS_DATA_DIR=./outputs/data

# Override individual DB paths (absolute or relative to PUSHPALS_DATA_DIR)
# PUSHPALS_DB_PATH=         # server DB (default: <data_dir>/pushpals.db)
# AGENT_REMOTE_DB_PATH=     # agent-remote idempotency DB (default: <data_dir>/agent-remote-state.db)

# ─── Server ──────────────────────────────────────────────────────────
# Port the PushPals server listens on (default 3001)
# PUSHPALS_PORT=3001

# Shared session ID — all apps (server, agents, client) join this session.
# Default: "dev" — everyone shares the same session with zero config.
# PUSHPALS_SESSION_ID=dev

# For Expo client, env vars must be prefixed with EXPO_PUBLIC_.
# Set this if you need the client to use a different session from agents.
# EXPO_PUBLIC_PUSHPALS_SESSION_ID=dev

# Optional shared auth token — if set, server + agents validate Bearer tokens
# PUSHPALS_AUTH_TOKEN=

# Set to 1/true/yes/on to enable verbose claim-poll request logging on the server
# DEBUG=1
# PUSHPALS_DEBUG_HTTP=1

# ─── LLM Provider (choose ONE section) ──────────────────────────────
#
# The agent-remote auto-detects which provider to use based on which
# API key / endpoint is set.  Priority: OPENAI → ANTHROPIC → LLM_ENDPOINT → Ollama.

# — OpenAI ─────────────────────────────────
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_API_ENDPOINT=https://api.openai.com

# — Anthropic ──────────────────────────────
# ANTHROPIC_API_KEY=
# ANTHROPIC_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_ENDPOINT=https://api.anthropic.com

# — Generic OpenAI-compatible (Ollama / vLLM / LM Studio) ──────────
# LLM_ENDPOINT is the base URL — the client appends /v1/chat/completions.
# OPENAI_API_KEY / ANTHROPIC_API_KEY must be unset for this to activate.
# LLM_ENDPOINT=http://localhost:11434
# LLM_MODEL=qwen3-coder:30b
# LLM_API_KEY=ollama

# ─── Agent-local planner (optional, separate from agent-remote LLM) ──
# PLANNER_ENDPOINT=http://localhost:11434/api/chat
# LLM_MODEL=qwen3-coder:30b
# PLANNER_API_KEY=ollama

# ─── Serial-pusher ───────────────────────────────────────────────────
# Skip the clean-repo guard (for dev working copies). Also available via --skip-clean-check.
# SERIAL_PUSHER_SKIP_CLEAN_CHECK=1

# Worker executor (OpenHands wrapper)
# WORKER_EXECUTOR=openhands
# WORKER_OPENHANDS_PYTHON=python
# WORKER_OPENHANDS_WORKSPACE_PYTHON=python3
# WORKER_OPENHANDS_TIMEOUT_MS=120000
# WORKER_REQUIRE_DOCKER=1
# WORKER_DOCKER_IMAGE=pushpals-worker-sandbox:latest
