# ─── PushPals Environment ─────────────────────────────────────────────
# Copy to .env and fill in values.  Bun auto-loads .env at the repo root.
# ─────────────────────────────────────────────────────────────────────

# ─── Server ──────────────────────────────────────────────────────────
# Port the PushPals server listens on (default 3001)
# PUSHPALS_PORT=3001

# Path to the server SQLite database (default pushpals.db, relative to apps/server/)
# PUSHPALS_DB_PATH=pushpals.db

# Optional shared auth token — if set, server + agents validate Bearer tokens
# PUSHPALS_AUTH_TOKEN=

# Set to any value to enable verbose request logging on the server
# DEBUG=1

# ─── LLM Provider (choose ONE section) ──────────────────────────────
#
# The agent-remote auto-detects which provider to use based on which
# API key / endpoint is set.  Priority: OPENAI → ANTHROPIC → LLM_ENDPOINT → Ollama.

# — OpenAI ─────────────────────────────────
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_API_ENDPOINT=https://api.openai.com

# — Anthropic ──────────────────────────────
# ANTHROPIC_API_KEY=
# ANTHROPIC_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_ENDPOINT=https://api.anthropic.com

# — Generic OpenAI-compatible (Ollama / vLLM / LM Studio) ──────────
# LLM_ENDPOINT is the base URL — the client appends /v1/chat/completions.
# OPENAI_API_KEY / ANTHROPIC_API_KEY must be unset for this to activate.
# LLM_ENDPOINT=http://localhost:11434
# LLM_MODEL=llama3
# LLM_API_KEY=ollama

# ─── Agent-local planner (optional, separate from agent-remote LLM) ──
# PLANNER_ENDPOINT=http://localhost:11434/api/chat
# PLANNER_MODEL=llama3
# PLANNER_API_KEY=
